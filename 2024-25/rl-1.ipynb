{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Copyright **`(c)`** 2024 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free under certain conditions â€” see the [`license`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "from tqdm.auto import tqdm\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 3\n",
    "GOAL_STATE = tuple(list(range(1, DIM**2)) + [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1|2|3\n",
      "4|5|6\n",
      "7|8|0\n"
     ]
    }
   ],
   "source": [
    "def print_board(board):\n",
    "    r\"\"\"Print board\"\"\"\n",
    "    print(\"\\n\".join(\"|\".join(f'{i:1}' for i in board[r * DIM : (r + 1) * DIM]) for r in range(DIM)))\n",
    "\n",
    "\n",
    "print_board(GOAL_STATE)\n",
    "\n",
    "\n",
    "def i2rc(i):\n",
    "    r\"\"\"Convert index to (row, column)\"\"\"\n",
    "    return divmod(i, DIM)\n",
    "\n",
    "\n",
    "def rc2i(r, c):\n",
    "    r\"\"\"Convert (row, column) to index\"\"\"\n",
    "    return r * DIM + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| state: (0, 0), actions: [(1, 0), (0, 1)]\n",
      "ic| state: (0, 1), actions: [(1, 1), (0, 0), (0, 2)]\n",
      "ic| state: (0, 2), actions: [(1, 2), (0, 1)]\n",
      "ic| state: (1, 0), actions: [(0, 0), (2, 0), (1, 1)]\n",
      "ic| state: (1, 1), actions: [(0, 1), (2, 1), (1, 0), (1, 2)]\n",
      "ic| state: (1, 2), actions: [(0, 2), (2, 2), (1, 1)]\n",
      "ic| state: (2, 0), actions: [(1, 0), (2, 1)]\n",
      "ic| state: (2, 1), actions: [(1, 1), (2, 0), (2, 2)]\n",
      "ic| state: (2, 2), actions: [(1, 2), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "ACTIONS = list()\n",
    "for i in range(DIM**2):\n",
    "    valid = list()\n",
    "    r, c = i2rc(i)\n",
    "    if r > 0:\n",
    "        valid.append(rc2i(r - 1, c))\n",
    "    if r < DIM - 1:\n",
    "        valid.append(rc2i(r + 1, c))\n",
    "    if c > 0:\n",
    "        valid.append(rc2i(r, c - 1))\n",
    "    if c < DIM - 1:\n",
    "        valid.append(rc2i(r, c + 1))\n",
    "    ACTIONS.append(tuple(valid))\n",
    "\n",
    "for i, a in enumerate(ACTIONS):\n",
    "    actions = [i2rc(_) for _ in a]\n",
    "    state = i2rc(i)\n",
    "    ic(state, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_action(state, action):\n",
    "    r\"\"\"Perform action in state, returns new state\"\"\"\n",
    "    new_state = list(state)\n",
    "    new_state[state.index(0)] = new_state[action]\n",
    "    new_state[action] = 0\n",
    "    return tuple(new_state)\n",
    "\n",
    "\n",
    "def get_possible_actions(state, final=GOAL_STATE):\n",
    "    r\"\"\"Gets a list of sction/reward for current state\"\"\"\n",
    "    if state == final:\n",
    "        return [(state.index(0), 0)]\n",
    "    else:\n",
    "        return [(a, -1) for a in ACTIONS[state.index(0)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state, value):\n",
    "    r\"\"\"Given a state a a value function, returns list of optimal greedy moves with associated immediate reward\"\"\"\n",
    "    q = dict()\n",
    "    for a, r in get_possible_actions(state):\n",
    "        new_state = do_action(state, a)\n",
    "        q[a] = r + value[new_state]\n",
    "    max_v = max(q.values())\n",
    "    return set((a, -1 if v < 0 else 0) for a, v in q.items() if v == max_v)\n",
    "\n",
    "\n",
    "def random_policy(state):\n",
    "    r\"\"\"Given a state a a value function, returns list of optimal greedy moves with associated immediate reward\"\"\"\n",
    "    return set(get_possible_actions(state))\n",
    "\n",
    "\n",
    "def describe(value):\n",
    "    for i in range(1, 1 - int(min(value.values()))):\n",
    "        s = [s for s, v in value.items() if v == -i]\n",
    "        if s:\n",
    "            print(f\"Found {len(s):,} states at distance {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reachability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 181440 states out of 362880\n"
     ]
    }
   ],
   "source": [
    "fronteer = [GOAL_STATE]\n",
    "reachable = set()\n",
    "while fronteer:\n",
    "    s = fronteer.pop()\n",
    "    reachable.add(s)\n",
    "    for a, _ in get_possible_actions(s, final=None):\n",
    "        ns = do_action(s, a)\n",
    "        if ns not in reachable:\n",
    "            reachable.add(ns)\n",
    "            fronteer.append(ns)\n",
    "print(f\"Found {len(reachable)} states out of {len(list(permutations(range(DIM**2))))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v() contains 12 states\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cb7711df1f4682987f4fc859fce711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{(0, 1, 3, 2): -19.99999999999997,\n",
       " (0, 2, 1, 3): -19.99999999999997,\n",
       " (0, 3, 2, 1): -35.99999999999994,\n",
       " (1, 0, 3, 2): -10.999999999999986,\n",
       " (1, 2, 0, 3): -10.999999999999986,\n",
       " (1, 2, 3, 0): 0.0,\n",
       " (2, 0, 1, 3): -26.99999999999996,\n",
       " (2, 3, 0, 1): -34.99999999999994,\n",
       " (2, 3, 1, 0): -31.99999999999995,\n",
       " (3, 0, 2, 1): -34.99999999999994,\n",
       " (3, 1, 0, 2): -26.99999999999996,\n",
       " (3, 1, 2, 0): -31.99999999999995}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = {tuple(s): 0 for s in permutations(range(DIM**2)) if tuple(s) in reachable}\n",
    "print(f\"v() contains {len(value):,} states\")\n",
    "\n",
    "current_policy = {s: random_policy(s) for s in value.keys()}\n",
    "for steps in tqdm(range(1000)):\n",
    "    new_value = dict()\n",
    "    for state in value:\n",
    "        new_value[state] = 0\n",
    "        actions = current_policy[state]\n",
    "        for a, r in actions:\n",
    "            new_value[state] += 1 / len(actions) * (r + value[do_action(state, a)])\n",
    "    value = new_value\n",
    "\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v() contains 181,440 states\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc55684dfbd4e189351238044b66f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 states at distance 1\n",
      "Found 4 states at distance 2\n",
      "Found 8 states at distance 3\n",
      "Found 16 states at distance 4\n",
      "Found 20 states at distance 5\n",
      "Found 39 states at distance 6\n",
      "Found 62 states at distance 7\n",
      "Found 116 states at distance 8\n",
      "Found 152 states at distance 9\n",
      "Found 286 states at distance 10\n",
      "Found 396 states at distance 11\n",
      "Found 748 states at distance 12\n",
      "Found 1,024 states at distance 13\n",
      "Found 1,893 states at distance 14\n",
      "Found 2,512 states at distance 15\n",
      "Found 4,485 states at distance 16\n",
      "Found 5,638 states at distance 17\n",
      "Found 9,529 states at distance 18\n",
      "Found 10,878 states at distance 19\n",
      "Found 16,993 states at distance 20\n",
      "Found 17,110 states at distance 21\n",
      "Found 23,952 states at distance 22\n",
      "Found 20,224 states at distance 23\n",
      "Found 24,047 states at distance 24\n",
      "Found 13,926 states at distance 25\n",
      "Found 14,560 states at distance 26\n",
      "Found 6,274 states at distance 27\n",
      "Found 3,720 states at distance 28\n",
      "Found 570 states at distance 29\n",
      "Found 133 states at distance 30\n"
     ]
    }
   ],
   "source": [
    "value = {tuple(s): 0 for s in permutations(range(DIM**2)) if tuple(s) in reachable}\n",
    "print(f\"v() contains {len(value):,} states\")\n",
    "\n",
    "current_policy = dict()\n",
    "new_policy = {s: random_policy(s) for s in value.keys()}\n",
    "\n",
    "stopping_condition = False\n",
    "steps = 0\n",
    "with tqdm() as pbar:\n",
    "    while not stopping_condition:\n",
    "        steps += 1\n",
    "        current_policy = new_policy\n",
    "        new_value = dict()\n",
    "        for state in value:\n",
    "            new_value[state] = 0\n",
    "            actions = current_policy[state]\n",
    "            for a, r in actions:\n",
    "                new_value[state] += 1 / len(actions) * (r + value[do_action(state, a)])\n",
    "        epsilon = max(abs(new_value[s]-value[s]) for s in value)\n",
    "        value = new_value\n",
    "        new_policy = {s: greedy_policy(s, value) for s in value.keys()}\n",
    "        pbar.update(1)\n",
    "        if epsilon < 1e-10:\n",
    "            stopping_condition = True\n",
    "describe(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v() contains 181,440 states\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8a40dc7e4b4468a694f73f0a4d0b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 states at distance 1\n",
      "Found 4 states at distance 2\n",
      "Found 8 states at distance 3\n",
      "Found 16 states at distance 4\n",
      "Found 20 states at distance 5\n",
      "Found 39 states at distance 6\n",
      "Found 62 states at distance 7\n",
      "Found 116 states at distance 8\n",
      "Found 152 states at distance 9\n",
      "Found 286 states at distance 10\n",
      "Found 396 states at distance 11\n",
      "Found 748 states at distance 12\n",
      "Found 1,024 states at distance 13\n",
      "Found 1,893 states at distance 14\n",
      "Found 2,512 states at distance 15\n",
      "Found 4,485 states at distance 16\n",
      "Found 5,638 states at distance 17\n",
      "Found 9,529 states at distance 18\n",
      "Found 10,878 states at distance 19\n",
      "Found 16,993 states at distance 20\n",
      "Found 17,110 states at distance 21\n",
      "Found 23,952 states at distance 22\n",
      "Found 20,224 states at distance 23\n",
      "Found 24,047 states at distance 24\n",
      "Found 13,926 states at distance 25\n",
      "Found 14,560 states at distance 26\n",
      "Found 6,274 states at distance 27\n",
      "Found 3,720 states at distance 28\n",
      "Found 570 states at distance 29\n",
      "Found 133 states at distance 30\n"
     ]
    }
   ],
   "source": [
    "value = {tuple(s): 0 for s in permutations(range(DIM**2)) if tuple(s) in reachable}\n",
    "print(f\"v() contains {len(value):,} states\")\n",
    "\n",
    "current_policy = dict()\n",
    "new_policy = {s: random_policy(s) for s in value.keys()}\n",
    "\n",
    "stopping_condition = False\n",
    "steps = 0\n",
    "with tqdm() as pbar:\n",
    "    while not stopping_condition:\n",
    "        steps += 1\n",
    "        current_policy = new_policy\n",
    "        for state in value:\n",
    "            value[state] = 0\n",
    "            actions = current_policy[state]\n",
    "            for a, r in actions:\n",
    "                value[state] += 1 / len(actions) * (r + value[do_action(state, a)])\n",
    "        new_policy = {s: greedy_policy(s, value) for s in value.keys()}\n",
    "        pbar.update(1)\n",
    "        if steps > 32:\n",
    "            stopping_condition = True\n",
    "describe(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-WEKR9SVn-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
